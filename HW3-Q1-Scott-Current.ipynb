{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trigrams(x):\n",
    "    res = []\n",
    "    n_grams = list(zip(*[x[i:] for i in range(3)]))\n",
    "    for n_gram in n_grams:\n",
    "        res.append(' '.join(n_gram))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [\"I don't know\", \"don't know who\", 'know who could', 'who could find', 'could find fault', 'find fault with', 'fault with a', 'with a simply', 'a simply human', 'simply human and', 'human and funny', 'and funny film', 'funny film like', 'film like this', 'like this with', 'this with lots', 'with lots of', 'lots of delights', 'of delights for', 'delights for your', 'for your heart.', 'your heart. I', 'heart. I enjoyed', 'I enjoyed each', 'enjoyed each minute', 'each minute of', 'minute of it', 'of it and', 'it and guessed', 'and guessed the', 'guessed the ending', 'the ending half', 'ending half way', 'half way through', 'way through the', 'through the movie', 'the movie --', 'movie -- but', '-- but that', 'but that did', 'that did not', 'did not disappoint', 'not disappoint me', 'disappoint me at', 'me at all.', 'at all. It', 'all. It will', 'It will not', 'will not only', 'not only touch', 'only touch your', 'touch your heart', 'your heart but', \"heart but it's\", \"but it's such\", \"it's such a\", 'such a good', 'a good family', 'good family friendly', 'family friendly film--we', 'friendly film--we need', 'film--we need many', 'need many more', 'many more like', 'more like these!'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "##############importing imdb daasets#############\n",
    "TEXT = data.Field(preprocessing=generate_trigrams)\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT,LABEL)\n",
    "print(vars(train_data.examples[0]))\n",
    "train_data, valid_data = train_data.split(random_state=random.seed(SEED),split_ratio=0.9)\n",
    "TEXT.build_vocab(train_data)\n",
    "dictionary = dict(TEXT.vocab.freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data, datasets\n",
    "import nltk\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few helper functions\n",
    "def invert_dict(d):\n",
    "    d_inv = defaultdict(list)\n",
    "    for k, v in d.items():\n",
    "        d_inv[v].append(k)\n",
    "    return d_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markov n-gram doesn't need test data, so no need to split\n",
    "TEXT = data.Field(lower=True, tokenize='spacy')\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "all_data = datasets.IMDB.splits(TEXT, LABEL, test=None)\n",
    "all_data = all_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in all_data.examples:\n",
    "    for word in ex.text:\n",
    "        word = re.sub(r'[^A-Za-z.!?\\' ]', '', word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 25000\n",
      "['i', 'do', \"n't\", 'know', 'who', 'could', 'find', 'fault', 'with', 'a', 'simply', 'human', 'and', 'funny', 'film', 'like', 'this', 'with', 'lots', 'of', 'delights', 'for', 'your', 'heart', '.', 'i', 'enjoyed', 'each', 'minute', 'of', 'it', 'and', 'guessed', 'the', 'ending', 'half', 'way', 'through', 'the', 'movie', '--', 'but', 'that', 'did', 'not', 'disappoint', 'me', 'at', 'all', '.', 'it', 'will', 'not', 'only', 'touch', 'your', 'heart', 'but', 'it', \"'s\", 'such', 'a', 'good', 'family', 'friendly', 'film', '--', 'we', 'need', 'many', 'more', 'like', 'these', '!']\n"
     ]
    }
   ],
   "source": [
    "# View sample review\n",
    "print(f'Number of reviews: {len(all_data)}')\n",
    "print(all_data.examples[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = defaultdict(int)\n",
    "for ex in all_data.examples:\n",
    "    for word in ex.text:\n",
    "        if word[0] == \"'\":\n",
    "            contractions[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {\"'s\": 62090,\n",
       "             \"'m\": 4422,\n",
       "             \"'ll\": 2703,\n",
       "             \"'\": 17138,\n",
       "             \"'ve\": 4946,\n",
       "             \"'re\": 3653,\n",
       "             \"'d\": 2212,\n",
       "             \"'<br\": 29,\n",
       "             \"'cause\": 55,\n",
       "             \"'mac\": 1,\n",
       "             \"'---then\": 1,\n",
       "             \"'---the\": 1,\n",
       "             \"'em\": 73,\n",
       "             \"'bout\": 10,\n",
       "             \"'til\": 1,\n",
       "             \"'big\": 1,\n",
       "             \"''\": 9,\n",
       "             \"'in\": 1,\n",
       "             \"'coz\": 2,\n",
       "             \"'cos\": 5,\n",
       "             \"').<br\": 1,\n",
       "             \"'the\": 1,\n",
       "             \"'that\": 1,\n",
       "             \"'till\": 1,\n",
       "             '\\'\"<br': 1,\n",
       "             \"'nuff\": 2,\n",
       "             \"'do\": 1,\n",
       "             \"'looks\": 1,\n",
       "             \"'budget\": 1})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'lefts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-40645c86e35c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"'\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mcontractions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlefts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'lefts'"
     ]
    }
   ],
   "source": [
    "contractions = defaultdict(int)\n",
    "contr_prefix = defaultdict(list)\n",
    "for ex in all_data.examples:\n",
    "    for word in ex.text:\n",
    "        if word[0] == \"'\":\n",
    "            contractions[word.lefts()].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ngram dict from reviews\n",
    "ngrams = defaultdict(list) # allows new keys to be added to ngrams with extra boilerplate\n",
    "gram_size = 3\n",
    "\n",
    "#words_tokens = nltk.word_tokenize(article_text)\n",
    "for ex in all_data.examples:\n",
    "    words_tokens = ex.text\n",
    "    for i in range(len(words_tokens)-gram_size):\n",
    "        seq = ' '.join(words_tokens[i:i+gram_size])\n",
    "        #print(seq)\n",
    "        #if  seq not in ngrams.keys():\n",
    "        #    ngrams[seq] = []\n",
    "        ngrams[seq].append(words_tokens[i+gram_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def high_prob_word(ngram_key):\n",
    "ngram_key = 'my favorite movie'\n",
    "ngram_key = 'I liked the'\n",
    "#for word in ngrams[ngram_key]:\n",
    "freq_dict = invert_dict(Counter(ngrams[ngram_key]))\n",
    "freq_keys = sorted(freq_dict.keys(), reverse=True)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 7, 6, 5, 4, 3, 2, 1]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_keys = sorted(freq_dict.keys(), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextWord(words):\n",
    "    freq_dict = invert_dict(Counter(words))\n",
    "    freq_keys = sorted(freq_dict.keys(), reverse=True)\n",
    "    possible_words = freq_dict[freq_keys[0]]\n",
    "    next_word = possible_words[random.randrange(len(possible_words))]\n",
    "    return next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at', '!', 'of', 'of', 'of', 'starring', 'genre', 'list', 'and', 'as', 'genre', 'almost', 'of', 'of', 'ever', 'he', 'ever', 'of', ',', 'but', 'of', 'and', ',', '!', '.', 'growing', '.', 'of', '.', '.', 'of', 'of']\n"
     ]
    }
   ],
   "source": [
    "# Check what's available as next words for our seed phrase\n",
    "print(ngrams['my favorite movie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "favorite movie .\n",
      "movie . what\n",
      ". what 's\n",
      "what 's more\n",
      "'s more ,\n",
      "more , i\n",
      ", i decided\n",
      "i decided to\n",
      "decided to give\n",
      "to give ``\n",
      "bad seq\n",
      "give `` suck\n",
      "bad seq\n",
      "`` suck wait\n",
      "bad seq\n",
      "suck wait write\n",
      "bad seq\n",
      "wait write give\n",
      "bad seq\n",
      "write give check\n",
      "bad seq\n",
      "give check revisit\n",
      "bad seq\n",
      "check revisit watch\n",
      "bad seq\n",
      "revisit watch check\n",
      "bad seq\n",
      "watch check do\n",
      "bad seq\n",
      "check do go\n",
      "bad seq\n",
      "do go watch\n",
      "bad seq\n",
      "go watch rent\n",
      "bad seq\n",
      "watch rent revisit\n",
      "bad seq\n",
      "rent revisit continue\n",
      "bad seq\n",
      "revisit continue watch\n",
      "bad seq\n",
      "continue watch give\n",
      "bad seq\n",
      "watch give give\n",
      "bad seq\n",
      "give give watch\n",
      "bad seq\n",
      "give watch go\n",
      "bad seq\n",
      "watch go see\n",
      "bad seq\n",
      "go see never\n",
      "bad seq\n",
      "see never see\n",
      "bad seq\n",
      "never see stay\n",
      "bad seq\n",
      "see stay watch\n",
      "bad seq\n",
      "stay watch check\n",
      "bad seq\n",
      "watch check check\n",
      "bad seq\n",
      "check check pick\n",
      "bad seq\n",
      "check pick take\n",
      "bad seq\n",
      "pick take give\n",
      "bad seq\n",
      "take give watch\n",
      "bad seq\n",
      "give watch give\n",
      "bad seq\n",
      "watch give check\n",
      "bad seq\n",
      "give check go\n",
      "bad seq\n",
      "check go buy\n",
      "bad seq\n",
      "go buy take\n",
      "bad seq\n",
      "buy take put\n",
      "bad seq\n",
      "take put watch\n",
      "bad seq\n",
      "50 my favorite movie . what 's more , i decided to give \" suck wait write give check revisit watch check do go watch rent revisit continue watch give give watch go see never see stay watch check check pick take give watch give check go buy take put watch\n"
     ]
    }
   ],
   "source": [
    "curr_sequence = 'my favorite movie'\n",
    "output = curr_sequence\n",
    "#for i in range(17):\n",
    "while len(output.split(sep=' ')) < 50:\n",
    "    possible_words = ngrams[curr_sequence]\n",
    "    next_word = possible_words[random.randrange(len(possible_words))]\n",
    "    output += ' ' + next_word\n",
    "    seq_words = nltk.word_tokenize(output)\n",
    "    next_sequence = ' '.join(seq_words[len(seq_words)-gram_size:len(seq_words)])\n",
    "    print(next_sequence)\n",
    "    if next_sequence in ngrams.keys():\n",
    "        prev_sequence = curr_sequence\n",
    "        curr_sequence = next_sequence\n",
    "    else:\n",
    "        print('bad seq')\n",
    "        curr_sequence = prev_sequence\n",
    "        continue\n",
    "\n",
    "print(len(output.split(sep=' ')), output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my',\n",
       " 'favorite',\n",
       " 'movie',\n",
       " 'he',\n",
       " 'has',\n",
       " 'ever',\n",
       " 'been',\n",
       " 'released',\n",
       " 'on',\n",
       " 'dvd',\n",
       " '.',\n",
       " 'video',\n",
       " '.',\n",
       " 'video',\n",
       " 'video',\n",
       " 'dvd',\n",
       " '.',\n",
       " 'video',\n",
       " '.',\n",
       " 'dvd',\n",
       " 'dvd',\n",
       " '--',\n",
       " 'this',\n",
       " 'dvd',\n",
       " 'dvd',\n",
       " '--',\n",
       " 'this',\n",
       " 'dvd']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_sequence = 'my favorite movie'\n",
    "output = curr_sequence\n",
    "#for i in range(17):\n",
    "while len(curr_sequence) < 20:\n",
    "    if curr_sequence not in ngrams.keys():\n",
    "        break\n",
    "    possible_words = ngrams[curr_sequence]\n",
    "    next_word = possible_words[random.randrange(len(possible_words))]\n",
    "    #next_word = nextWord(possible_words)\n",
    "    output += ' ' + next_word\n",
    "    seq_words = nltk.word_tokenize(output)\n",
    "    next_sequence = ' '.join(seq_words[len(seq_words)-words:len(seq_words)])\n",
    "    \n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
