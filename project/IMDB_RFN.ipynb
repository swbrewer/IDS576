{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Convolutional Sentiment Analysis\n",
    "\n",
    "In the previous notebooks, we managed to achieve a test accuracy of ~85% using RNNs and an implementation of the [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759) model. In this notebook, we will be using a *convolutional neural network* (CNN) to conduct sentiment analysis, implementing the model from [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882).\n",
    "\n",
    "**Note**: This tutorial is not aiming to give a comprehensive introduction and explanation of CNNs. For a better and more in-depth explanation check out [here](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/) and [here](https://cs231n.github.io/convolutional-networks/).\n",
    "\n",
    "Traditionally, CNNs are used to analyse images and are made up of one or more *convolutional* layers, followed by one or more linear layers. The convolutional layers use filters (also called *kernels* or *receptive fields*) which scan across an image and produce a processed version of the image. This processed version of the image can be fed into another convolutional layer or a linear layer. Each filter has a shape, e.g. a 3x3 filter covers a 3 pixel wide and 3 pixel high area of the image, and each element of the filter has a weight associated with it, the 3x3 filter would have 9 weights. In traditional image processing these weights were specified by hand by engineers, however the main advantage of the convolutional layers in neural networks is that these weights are learned via backpropagation. \n",
    "\n",
    "The intuitive idea behind learning the weights is that your convolutional layers act like *feature extractors*, extracting parts of the image that are most important for your CNN's goal, e.g. if using a CNN to detect faces in an image, the CNN may be looking for features such as the existance of a nose, mouth or a pair of eyes in the image.\n",
    "\n",
    "So why use CNNs on text? In the same way that a 3x3 filter can look over a patch of an image, a 1x2 filter can look over a 2 sequential words in a piece of text, i.e. a bi-gram. In the previous tutorial we looked at the FastText model which used bi-grams by explicitly adding them to the end of a text, in this CNN model we will instead use multiple filters of different sizes which will look at the bi-grams (a 1x2 filter), tri-grams (a 1x3 filter) and/or n-grams (a 1x$n$ filter) within the text.\n",
    "\n",
    "The intuition here is that the appearance of certain bi-grams, tri-grams and n-grams within the review will be a good indication of the final sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "As in the previous notebooks, we'll prepare the data. \n",
    "\n",
    "Unlike the previous notebook with the FastText model, we no longer explicitly need to create the bi-grams and append them to the end of the sentence.\n",
    "\n",
    "As convolutional layers expect the batch dimension to be first we can tell TorchText to return the data already permuted using the `batch_first = True` argument on the field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext import vocab\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy', batch_first = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL, root='~/IDS576/data/')\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the vocab and load the pre-trained word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "glove_path = '/home/team3/IDS576/glove/'\n",
    "glove_vectors = vocab.Vectors('glove.6B.100d.txt', glove_path)\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = glove_vectors, \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we create the iterators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "\n",
    "Now to build our model.\n",
    "\n",
    "The first major hurdle is visualizing how CNNs are used for text. Images are typically 2 dimensional (we'll ignore the fact that there is a third \"colour\" dimension for now) whereas text is 1 dimensional. However, we know that the first step in almost all of our previous tutorials (and pretty much all NLP pipelines) is converting the words into word embeddings. This is how we can visualize our words in 2 dimensions, each word along one axis and the elements of vectors aross the other dimension. Consider the 2 dimensional representation of the embedded sentence below:\n",
    "\n",
    "![](assets/sentiment9.png)\n",
    "\n",
    "We can then use a filter that is **[n x emb_dim]**. This will cover $n$ sequential words entirely, as their width will be `emb_dim` dimensions. Consider the image below, with our word vectors are represented in green. Here we have 4 words with 5 dimensional embeddings, creating a [4x5] \"image\" tensor. A filter that covers two words at a time (i.e. bi-grams) will be **[2x5]** filter, shown in yellow, and each element of the filter with have a _weight_ associated with it. The output of this filter (shown in red) will be a single real number that is the weighted sum of all elements covered by the filter.\n",
    "\n",
    "![](assets/sentiment12.png)\n",
    "\n",
    "The filter then moves \"down\" the image (or across the sentence) to cover the next bi-gram and another output (weighted sum) is calculated. \n",
    "\n",
    "![](assets/sentiment13.png)\n",
    "\n",
    "Finally, the filter moves down again and the final output for this filter is calculated.\n",
    "\n",
    "![](assets/sentiment14.png)\n",
    "\n",
    "In our case (and in the general case where the width of the filter equals the width of the \"image\"), our output will be a vector with number of elements equal to the height of the image (or lenth of the word) minus the height of the filter plus one, $4-2+1=3$ in this case.\n",
    "\n",
    "This example showed how to calculate the output of one filter. Our model (and pretty much all CNNs) will have lots of these filters. The idea is that each filter will learn a different feature to extract. In the above example, we are hoping each of the **[2 x emb_dim]** filters will be looking for the occurence of different bi-grams. \n",
    "\n",
    "In our model, we will also have different sizes of filters, heights of 3, 4 and 5, with 100 of each of them. The intuition is that we will be looking for the occurence of different tri-grams, 4-grams and 5-grams that are relevant for analysing sentiment of movie reviews.\n",
    "\n",
    "The next step in our model is to use *pooling* (specifically *max pooling*) on the output of the convolutional layers. This is similar to the FastText model where we performed the average over each of the word vectors, implemented by the `F.avg_pool2d` function, however instead of taking the average over a dimension, we are taking the maximum value over a dimension. Below an example of taking the maximum value (0.9) from the output of the convolutional layer on the example sentence (not shown is the activation function applied to the output of the convolutions).\n",
    "\n",
    "![](assets/sentiment15.png)\n",
    "\n",
    "The idea here is that the maximum value is the \"most important\" feature for determining the sentiment of the review, which corresponds to the \"most important\" n-gram within the review. How do we know what the \"most important\" n-gram is? Luckily, we don't have to! Through backpropagation, the weights of the filters are changed so that whenever certain n-grams that are highly indicative of the sentiment are seen, the output of the filter is a \"high\" value. This \"high\" value then passes through the max pooling layer if it is the maximum value in the output. \n",
    "\n",
    "As our model has 100 filters of 3 different sizes, that means we have 300 different n-grams the model thinks are important. We concatenate these together into a single vector and pass them through a linear layer to predict the sentiment. We can think of the weights of this linear layer as \"weighting up the evidence\" from each of the 300 n-grams and making a final decision. \n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "We implement the convolutional layers with `nn.Conv2d`. The `in_channels` argument is the number of \"channels\" in your image going into the convolutional layer. In actual images this is usually 3 (one channel for each of the red, blue and green channels), however when using text we only have a single channel, the text itself. The `out_channels` is the number of filters and the `kernel_size` is the size of the filters. Each of our `kernel_size`s is going to be **[n x emb_dim]** where $n$ is the size of the n-grams.\n",
    "\n",
    "In PyTorch, RNNs want the input with the batch dimension second, whereas CNNs want the batch dimension first - we do not have to permute the data here as we have already set `batch_first = True` in our `TEXT` field. We then pass the sentence through an embedding layer to get our embeddings. The second dimension of the input into a `nn.Conv2d` layer must be the channel dimension. As text technically does not have a channel dimension, we `unsqueeze` our tensor to create one. This matches with our `in_channels=1` in the initialization of our convolutional layers. \n",
    "\n",
    "We then pass the tensors through the convolutional and pooling layers, using the `ReLU` activation function after the convolutional layers. Another nice feature of the pooling layers is that they handle sentences of different lengths. The size of the output of the convolutional layer is dependent on the size of the input to it, and different batches contain sentences of different lengths. Without the max pooling layer the input to our linear layer would depend on the size of the input sentence (not what we want). One option to rectify this would be to trim/pad all sentences to the same length, however with the max pooling layer we always know the input to the linear layer will be the total number of filters. **Note**: there an exception to this if your sentence(s) are shorter than the largest filter used. You will then have to pad your sentences to the length of the largest filter. In the IMDb data there are no reviews only 5 words long so we don't have to worry about that, but you will if you are using your own data.\n",
    "\n",
    "Finally, we perform dropout on the concatenated filter outputs and then pass them through a linear layer to make our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the `CNN` model can only use 3 different sized filters, but we can actually improve the code of our model to make it more generic and take any number of filters.\n",
    "\n",
    "We do this by placing all of our convolutional layers in a  `nn.ModuleList`, a function used to hold a list of PyTorch `nn.Module`s. If we simply used a standard Python list, the modules within the list cannot be \"seen\" by any modules outside the list which will cause us some errors.\n",
    "\n",
    "We can now pass an arbitrary sized list of filter sizes and the list comprehension will create a convolutional layer for each of them. Then, in the `forward` method we iterate through the list applying each convolutional layer to get a list of convolutional outputs, which we also feed through the max pooling in a list comprehension before concatenating together and passing through the dropout and linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, rfn, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.rfn = rfn\n",
    "                \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        #text = [batch size, sent len]\n",
    "        text = self.rfn(text)\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "                \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class RFN(nn.Module):\n",
    "    '''Wrapper class that can be inserted into a nn.Module based model to add an RFN layer'''\n",
    "    \n",
    "    \n",
    "    def __init__(self, mu=.1, sigma=.05, rfn_flag=True):\n",
    "        super(RFN, self).__init__()\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.rfn_flag = rfn_flag\n",
    "        self.shape_vals = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        if self.rfn_flag:\n",
    "            print('x', x.dtype)\n",
    "            print('rfn_x', self.random_masks(x).dtype)\n",
    "            x = self.random_masks(x) * x\n",
    "            x = x.long()\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def random_masks(self, x):\n",
    "        self.shape_vals = x.size(), x[0].size(), x[0].nelement()\n",
    "        #print(self.shape_vals)\n",
    "\n",
    "        # Create ones array of size (batch,channel,horiz,vert)\n",
    "        masks = np.ones(self.shape_vals[0])\n",
    "\n",
    "        # Create random masks for each sample in the batch\n",
    "        for mask in masks:\n",
    "            zero_cnt = np.absolute(int(np.ceil((self.sigma * np.random.randn() + self.mu) * self.shape_vals[2])))\n",
    "            zero_idx = np.random.choice(self.shape_vals[2], zero_cnt, replace=False)\n",
    "            mask.reshape(self.shape_vals[2])[zero_idx] = 0\n",
    "            mask.reshape(self.shape_vals[1])\n",
    "        return torch.tensor(masks, dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also implement the above model using 1-dimensional convolutional layers, where the embedding dimension is the \"depth\" of the filter and the number of tokens in the sentence is the width.\n",
    "\n",
    "We'll run our tests in this notebook using the 2-dimensional convolutional model, but leave the implementation for the 1-dimensional model below for anyone interested. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of our `CNN` class. \n",
    "\n",
    "We can change `CNN` to `CNN1d` if we want to run the 1-dimensional convolutional model, noting that both models give almost identical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "rfn = RFN(mu=.1, sigma=.05, rfn_flag=True)\n",
    "rfn.to(device)\n",
    "\n",
    "model = CNN(rfn, INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "rfn = RFN(mu=.1, sigma=.05, rfn_flag=True)\n",
    "rfn.to(device)\n",
    "\n",
    "model = CNN(rfn, INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25002"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the number of parameters in our model we can see it has about the same as the FastText model. \n",
    "\n",
    "Both the `CNN` and the `CNN1d` models have the exact same number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,620,801 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load the pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [-0.1983, -0.2634,  0.4227,  ...,  0.1574, -0.3458,  0.4537],\n",
       "        [-0.0640,  0.6652, -0.4537,  ..., -0.4337, -0.7348, -0.2443],\n",
       "        [-0.2821,  0.0417,  0.4807,  ..., -0.5425, -0.7024,  1.3024]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then zero the initial weights of the unknown and padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is the same as before. We initialize the optimizer, loss function (criterion) and place the model and criterion on the GPU (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the function to calculate accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function for training our model...\n",
    "\n",
    "**Note**: as we are using dropout again, we must remember to use `model.train()` to ensure the dropout is \"turned on\" while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function for testing our model...\n",
    "\n",
    "**Note**: again, as we are now using dropout, we must remember to use `model.eval()` to ensure the dropout is \"turned off\" while evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our function to tell us how long epochs take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(mu, rfn_flag):\n",
    "    INPUT_DIM = len(TEXT.vocab)\n",
    "    EMBEDDING_DIM = 100\n",
    "    N_FILTERS = 100\n",
    "    FILTER_SIZES = [3,4,5]\n",
    "    OUTPUT_DIM = 1\n",
    "    DROPOUT = 0.5\n",
    "    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "    rfn = RFN(mu=mu, sigma=.05, rfn_flag=rfn_flag)\n",
    "    rfn.to(device)\n",
    "\n",
    "    model = CNN(rfn, INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "\n",
    "    pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "    UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RFN, mu=0:\n",
      "False\n",
      "Epoch: 01 | Epoch Time: 0m 19s\n",
      "\tTrain Loss: 0.501 | Train Acc: 74.73%\n",
      "\t Val. Loss: 0.336 |  Val. Acc: 86.15%\n",
      "Epoch: 02 | Epoch Time: 0m 19s\n",
      "\tTrain Loss: 0.305 | Train Acc: 87.34%\n",
      "\t Val. Loss: 0.284 |  Val. Acc: 88.22%\n",
      "Epoch: 03 | Epoch Time: 0m 19s\n",
      "\tTrain Loss: 0.221 | Train Acc: 91.48%\n",
      "\t Val. Loss: 0.259 |  Val. Acc: 89.22%\n",
      "Epoch: 04 | Epoch Time: 0m 19s\n",
      "\tTrain Loss: 0.148 | Train Acc: 94.82%\n",
      "\t Val. Loss: 0.277 |  Val. Acc: 89.02%\n",
      "Epoch: 05 | Epoch Time: 0m 20s\n",
      "\tTrain Loss: 0.088 | Train Acc: 97.06%\n",
      "\t Val. Loss: 0.292 |  Val. Acc: 89.26%\n",
      "Training RFN, mu=0.1:\n",
      "True\n",
      "Epoch: 01 | Epoch Time: 0m 20s\n",
      "\tTrain Loss: 0.511 | Train Acc: 73.56%\n",
      "\t Val. Loss: 0.358 |  Val. Acc: 84.67%\n",
      "Epoch: 02 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.333 | Train Acc: 85.61%\n",
      "\t Val. Loss: 0.312 |  Val. Acc: 86.53%\n",
      "Epoch: 03 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.260 | Train Acc: 89.66%\n",
      "\t Val. Loss: 0.278 |  Val. Acc: 88.41%\n",
      "Epoch: 04 | Epoch Time: 0m 21s\n",
      "\tTrain Loss: 0.195 | Train Acc: 92.68%\n",
      "\t Val. Loss: 0.269 |  Val. Acc: 88.78%\n",
      "Epoch: 05 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.147 | Train Acc: 94.57%\n",
      "\t Val. Loss: 0.302 |  Val. Acc: 87.92%\n",
      "Training RFN, mu=0.2:\n",
      "True\n",
      "Epoch: 01 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.530 | Train Acc: 72.29%\n",
      "\t Val. Loss: 0.369 |  Val. Acc: 84.30%\n",
      "Epoch: 02 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.361 | Train Acc: 84.36%\n",
      "\t Val. Loss: 0.320 |  Val. Acc: 86.63%\n",
      "Epoch: 03 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.294 | Train Acc: 87.60%\n",
      "\t Val. Loss: 0.290 |  Val. Acc: 87.80%\n",
      "Epoch: 04 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.244 | Train Acc: 90.28%\n",
      "\t Val. Loss: 0.305 |  Val. Acc: 87.31%\n",
      "Epoch: 05 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.185 | Train Acc: 92.84%\n",
      "\t Val. Loss: 0.291 |  Val. Acc: 88.16%\n",
      "Training RFN, mu=0.3:\n",
      "True\n",
      "Epoch: 01 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.542 | Train Acc: 71.52%\n",
      "\t Val. Loss: 0.402 |  Val. Acc: 82.26%\n",
      "Epoch: 02 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.382 | Train Acc: 83.08%\n",
      "\t Val. Loss: 0.343 |  Val. Acc: 85.34%\n",
      "Epoch: 03 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.321 | Train Acc: 86.16%\n",
      "\t Val. Loss: 0.310 |  Val. Acc: 86.78%\n",
      "Epoch: 04 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.265 | Train Acc: 88.86%\n",
      "\t Val. Loss: 0.307 |  Val. Acc: 86.79%\n",
      "Epoch: 05 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.229 | Train Acc: 90.87%\n",
      "\t Val. Loss: 0.300 |  Val. Acc: 87.51%\n",
      "Training RFN, mu=0.4:\n",
      "True\n",
      "Epoch: 01 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.554 | Train Acc: 70.28%\n",
      "\t Val. Loss: 0.424 |  Val. Acc: 81.04%\n",
      "Epoch: 02 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.406 | Train Acc: 81.56%\n",
      "\t Val. Loss: 0.360 |  Val. Acc: 84.38%\n",
      "Epoch: 03 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.351 | Train Acc: 84.52%\n",
      "\t Val. Loss: 0.338 |  Val. Acc: 85.37%\n",
      "Epoch: 04 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.312 | Train Acc: 86.80%\n",
      "\t Val. Loss: 0.324 |  Val. Acc: 86.22%\n",
      "Epoch: 05 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.270 | Train Acc: 88.87%\n",
      "\t Val. Loss: 0.316 |  Val. Acc: 86.18%\n",
      "Training RFN, mu=0.5:\n",
      "True\n",
      "Epoch: 01 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.569 | Train Acc: 68.88%\n",
      "\t Val. Loss: 0.434 |  Val. Acc: 80.25%\n",
      "Epoch: 02 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.440 | Train Acc: 79.57%\n",
      "\t Val. Loss: 0.397 |  Val. Acc: 82.20%\n",
      "Epoch: 03 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.383 | Train Acc: 82.83%\n",
      "\t Val. Loss: 0.363 |  Val. Acc: 83.68%\n",
      "Epoch: 04 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.353 | Train Acc: 84.97%\n",
      "\t Val. Loss: 0.355 |  Val. Acc: 84.38%\n",
      "Epoch: 05 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.315 | Train Acc: 86.66%\n",
      "\t Val. Loss: 0.340 |  Val. Acc: 85.21%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "rfn_mu_vals = [0, .1, .2, .3, .4, .5]\n",
    "\n",
    "for mu in rfn_mu_vals:\n",
    "    print(f'Training RFN, mu={mu}:')\n",
    "    # Set whether RFN will be used in training\n",
    "    if mu == 0:\n",
    "        rfn_flag = False\n",
    "    else:\n",
    "        rfn_flag = True\n",
    "    \n",
    "    print(rfn_flag)\n",
    "    \n",
    "    model = load_model(mu, rfn_flag)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f'./models/imdb_rfn_{mu}.pt')\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train our model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get test results comparable to the previous 2 models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MU: 0 | Test Loss: 0.441 | Test Acc: 79.98%\n",
      "MU: 0.1 | Test Loss: 0.400 | Test Acc: 82.45%\n",
      "MU: 0.2 | Test Loss: 0.387 | Test Acc: 82.66%\n",
      "MU: 0.3 | Test Loss: 0.379 | Test Acc: 83.45%\n",
      "MU: 0.4 | Test Loss: 0.389 | Test Acc: 82.88%\n",
      "MU: 0.5 | Test Loss: 0.365 | Test Acc: 83.97%\n"
     ]
    }
   ],
   "source": [
    "rfn_mu_vals = [0, .1, .2, .3, .4, .5]\n",
    "for mu in rfn_mu_vals:\n",
    "    model.load_state_dict(torch.load(f'./models/imdb_rfn_{mu}.pt'))\n",
    "\n",
    "    test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "    print(f'MU: {mu} | Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input\n",
    "\n",
    "And again, as a sanity check we can check some input sentences\n",
    "\n",
    "**Note**: As mentioned in the implementation details, the input sentence has to be at least as long as the largest filter height used. We modify our `predict_sentiment` function to also accept a minimum length argument. If the tokenized input sentence is less than `min_len` tokens, we append padding tokens (`<pad>`) to make it `min_len` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "model.load_state_dict(torch.load(f'./models/imdb_rfn_{0.5}.pt'))\n",
    "\n",
    "def predict_sentiment(model, sentence, min_len = 5):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example negative review..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1745041310787201"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example positive review..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7319116592407227"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is great\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##FSGM Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_batch(net, orig, criterion, eps=15):\n",
    "    '''Given a pytorch CNN model, original image as tensor, and value for epsilon, create adversarial sample\n",
    "    using fast gradient sign method (FGSM) described in arXiv:1412.6572v3'''\n",
    "    # Create input tensor from original image, unsqeezing by a dim to account for model expected batch dim\n",
    "    # float cast is likely not needed after normalization, but as float is required for cuda execution,\n",
    "    # it's best to ensure float data\n",
    "    inp = orig.float().clone().detach().to(device).requires_grad_(True)\n",
    "    print(inp)\n",
    "    \n",
    "    # Make prediction prior to attack to use in calulating gradients\n",
    "    print('before prior', inp.dtype, inp.size())\n",
    "    #for sample in inp:\n",
    "    prior_out = net(inp)\n",
    "    #print(prior_out.size(), prior_out)\n",
    "    prior_pred = F.relu(prior_out)\n",
    "    #print(prior_pred.size(), prior_pred)\n",
    "\n",
    "    # Use prediction vector to compute loss and then compute gradients from loss to use in attack\n",
    "    loss = criterion(prior_out, prior_pred)\n",
    "    loss.backward()\n",
    "    print(loss.grad_fn)\n",
    "    # Actual FSGM method\n",
    "    #print(orig.min(), orig.max())\n",
    "    print(dir(inp.grad))\n",
    "    inp = inp + ((eps/255.0) * torch.sign(inp.grad.data))\n",
    "    inp = inp.clamp(min=orig.min(), max=orig.max())\n",
    "    #print(inp.min(), inp.max())\n",
    "\n",
    "    #inp = inp.long().clone().detach().to(device).requires_grad_(False)\n",
    "    \n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_evaluate(model, iterator, criterion, eps=15, adv=False):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        if adv:\n",
    "            print('Before FGSM',batch.text[0].dtype)\n",
    "            print(batch.label)\n",
    "            batch.text = fgsm_batch(model, batch.text, criterion, eps = eps)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions, batch.label)\n",
    "\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def fsgm_evaluate(net, dataloader, eps=15, adv=False):\\n    correct = 0\\n    count = 0\\n    for i, data in enumerate(dataloader, 0):\\n        # get the inputs; data is a list of [inputs, labels]\\n        inputs, labels = data\\n        inputs = inputs.to(device)\\n        labels = labels.to(device)\\n        if adv:\\n            inputs = fgsm_batch(net, inputs, eps = eps)\\n        preds = net(inputs)\\n        if torch.argmax(preds) == labels:\\n            correct += 1.0\\n        count += 1.0\\n        if not i % 5000:\\n            print(f'Sample: {i}, Running Accuracy: {(correct/count):5.3f}')\\n    acc = correct/count\\n    print(f'Accuracy over {int(count)} samples: {acc:5.3f}')\\n    return acc\""
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def fsgm_evaluate(net, dataloader, eps=15, adv=False):\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        if adv:\n",
    "            inputs = fgsm_batch(net, inputs, eps = eps)\n",
    "        preds = net(inputs)\n",
    "        if torch.argmax(preds) == labels:\n",
    "            correct += 1.0\n",
    "        count += 1.0\n",
    "        if not i % 5000:\n",
    "            print(f'Sample: {i}, Running Accuracy: {(correct/count):5.3f}')\n",
    "    acc = correct/count\n",
    "    print(f'Accuracy over {int(count)} samples: {acc:5.3f}')\n",
    "    return acc'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# Create history dict for all models with validation accuracy and loss per epoch\n",
    "accs_no_fsgm = defaultdict(list)\n",
    "accs_fsgm = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate IMDB RFN at epsilon=40, mu=0:\n",
      "Before FGSM torch.int64\n",
      "tensor([1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "tensor([[4.4290e+03, 1.7000e+01, 1.6850e+03,  ..., 1.8905e+04, 1.8000e+01,\n",
      "         9.7490e+03],\n",
      "        [6.6000e+01, 2.3000e+01, 1.9000e+01,  ..., 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00],\n",
      "        [6.6000e+01, 9.0000e+00, 2.7000e+01,  ..., 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00],\n",
      "        ...,\n",
      "        [6.6000e+01, 1.9000e+01, 3.9400e+02,  ..., 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00],\n",
      "        [0.0000e+00, 2.1000e+01, 3.2400e+02,  ..., 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00],\n",
      "        [1.5750e+03, 5.9000e+01, 4.1300e+02,  ..., 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00]], device='cuda:0', requires_grad=True)\n",
      "before prior torch.float32 torch.Size([64, 1387])\n",
      "x torch.float32\n",
      "rfn_x torch.float32\n",
      "<BinaryCrossEntropyWithLogitsBackward object at 0x7f000085a438>\n",
      "['__bool__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-429-36902e181d34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0madv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Evaluate IMDB RFN at epsilon={eps}, mu={mu}:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0maccs_fsgm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfgsm_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdb_rfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Evaluate IMDB RFN without FSGM, mu={mu}:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-426-b0f2d763cd4b>\u001b[0m in \u001b[0;36mfgsm_evaluate\u001b[0;34m(model, iterator, criterion, eps, adv)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Before FGSM'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfgsm_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-425-f9369410ce25>\u001b[0m in \u001b[0;36mfgsm_batch\u001b[0;34m(net, orig, criterion, eps)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#print(orig.min(), orig.max())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#print(inp.min(), inp.max())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "# load IMDB models\n",
    "#train_dl = DataLoader(trainset, batch_size=1)\n",
    "\n",
    "rfn_mu_vals = [0, .1, .2, .3, .4, .5]\n",
    "\n",
    "#epsilons = [10, 20, 30, 40]\n",
    "epsilons = [40]\n",
    "\n",
    "for eps in epsilons:\n",
    "    for adv in [True, False]:\n",
    "        for i, mu in enumerate(rfn_mu_vals):\n",
    "            imdb_rfn = load_model(mu=mu, rfn_flag=True)\n",
    "            imdb_rfn.load_state_dict(torch.load(f'./models/imdb_rfn_{mu}.pt'))\n",
    "\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            imdb_rfn = imdb_rfn.to(device)\n",
    "            criterion = criterion.to(device)\n",
    "            \n",
    "            if adv:\n",
    "                print(f'Evaluate IMDB RFN at epsilon={eps}, mu={mu}:')\n",
    "                accs_fsgm[mu] = fgsm_evaluate(imdb_rfn, train_iterator, criterion, eps=eps, adv=adv)\n",
    "            else:\n",
    "                print(f'Evaluate IMDB RFN without FSGM, mu={mu}:')\n",
    "                accs_no_fsgm[mu] = fgsm_evaluate(imdb_rfn, train_iterator, criterion, eps=eps, adv=adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MU\tIMDB\tFSGM eps=40\n",
      "0\t0.955\t0.955\n",
      "0.1\t 0.96\t 0.96\n",
      "0.2\t0.915\t0.915\n",
      "0.3\t 0.94\t 0.94\n",
      "0.4\t 0.92\t 0.92\n",
      "0.5\t0.896\t0.896\n"
     ]
    }
   ],
   "source": [
    "print(f'MU\\tIMDB\\tFSGM eps=40')\n",
    "\n",
    "for mu in rfn_mu_vals:\n",
    "    print(f'{mu}\\t{accs_no_fsgm[mu][1]:5.3}\\t{accs_no_fsgm[mu][1]:5.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1455942576166487, 0.955120894160584)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs_no_fsgm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.batch.Batch'> torch.Size([831])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1111])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1207])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1098])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([994])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1163])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([835])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1365])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1078])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1201])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([720])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([830])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1094])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([862])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1179])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1191])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1165])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1122])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1188])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1827])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1098])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1108])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([918])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1022])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1013])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1119])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1117])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1067])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([824])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([620])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([959])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1035])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([764])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1050])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1027])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1126])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1060])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1175])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1035])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1250])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1097])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1115])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([821])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1146])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([997])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1156])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([871])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1191])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1088])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([864])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([849])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1369])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([842])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([637])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1014])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1076])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1179])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1516])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([713])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([776])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([878])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1030])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1112])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1045])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([915])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1084])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1247])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1226])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1166])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([950])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([712])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1200])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1202])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1284])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([872])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1016])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1160])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([868])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([952])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1140])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([869])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1161])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([942])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([964])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1118])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1175])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([754])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1106])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1032])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1117])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1169])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([706])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1058])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([683])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1069])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([934])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([783])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1454])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1050])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([785])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([800])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1083])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1122])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1191])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([985])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1196])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1200])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1016])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([777])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1143])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([983])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1148])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([776])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([982])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1194])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([931])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1039])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1111])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([936])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1206])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([860])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([876])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1142])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1158])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([843])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([682])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([915])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1150])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1154])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([674])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1144])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([927])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([2789])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1557])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([852])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([771])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1196])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1212])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1285])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([964])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1197])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([790])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([958])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([883])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([1123])\n",
      "<class 'torchtext.data.batch.Batch'> torch.Size([901])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-257-bc2a308cf7e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \"\"\"\n\u001b[1;32m    236\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    print(type(batch), batch.text[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
